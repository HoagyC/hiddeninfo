{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "940e3dfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/hoagycunningham'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "BASE_PATH = os.path.dirname(sys.path[0])\n",
    "\n",
    "\n",
    "from CUB.models import Multimodel\n",
    "from CUB.inference import eval\n",
    "from CUB.configs import multi_inst_cfg\n",
    "from CUB.dataset import load_data\n",
    "\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b921e44",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/hoagycunningham/CUB_instance_masked/train.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m args \u001b[38;5;241m=\u001b[39m multi_inst_cfg\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Looking at each model's activations\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrain_data_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m load_data([val_data_path], args)\n\u001b[1;32m     21\u001b[0m test_loader \u001b[38;5;241m=\u001b[39m load_data([test_data_path], args)\n",
      "File \u001b[0;32m~/hiddeninfo/CUB/dataset.py:185\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(pkl_paths, args, resol, uncertain_label)\u001b[0m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     class_sparsity \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 185\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCUBDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpkl_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43muncertain_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattr_sparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_sparsity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_training:\n\u001b[1;32m    194\u001b[0m     drop_last \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;66;03m# drop last batch if it is smaller than batch_size\u001b[39;00m\n",
      "File \u001b[0;32m~/hiddeninfo/CUB/dataset.py:48\u001b[0m, in \u001b[0;36mCUBDataset.__init__\u001b[0;34m(self, pkl_file_paths, uncertain_label, image_dir, transform, attr_sparsity, attr_class_sparsity)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28many\u001b[39m([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m path) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m path) \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m pkl_file_paths])\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m pkl_file_paths:\n\u001b[0;32m---> 48\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mextend(pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m))\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;241m=\u001b[39m transform\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muncertain_label \u001b[38;5;241m=\u001b[39m uncertain_label\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/hoagycunningham/CUB_instance_masked/train.pkl'"
     ]
    }
   ],
   "source": [
    "# Setting up\n",
    "\n",
    "# List the model paths\n",
    "# May not want to load all models into RAM simultaneously\n",
    "model_with_two_joints_path = BASE_PATH + \"/big_run/multi_inst_joint/20230302-141428/final_model.pth\"\n",
    "seq_model_path = BASE_PATH + \"/big_run/seq_inst/20230224-124623/final_model.pth\"\n",
    "seq_sparse_model_path = BASE_PATH + \"big_run/seq_inst_sparse/20230227-183548/final_model.pth\"\n",
    "multi_path = BASE_PATH + \"/big_run/multimodel_inst/20230224-172742/final_model.pth\"\n",
    "ind_path = BASE_PATH + \"/big_run/ind_inst/20230224-135153/final_model.pth\"\n",
    "                        \n",
    "train_data_path = BASE_PATH + \"/CUB_instance_masked/train.pkl\"\n",
    "val_data_path = BASE_PATH + \"/CUB_instance_masked/val.pkl\"\n",
    "test_data_path = BASE_PATH + \"/CUB_instance_masked/test.pkl\"\n",
    "\n",
    "args = multi_inst_cfg\n",
    "\n",
    "# Looking at each model's activations\n",
    "\n",
    "train_loader = load_data([train_data_path], args)\n",
    "val_loader = load_data([val_data_path], args)\n",
    "test_loader = load_data([test_data_path], args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52192729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/big_run/seq_inst/20230224-124623/final_model.pth\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/big_run/seq_inst/20230224-124623/final_model.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_path, olist \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_paths, output_lists):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(model_path)\n\u001b[0;32m---> 14\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n",
      "File \u001b[0;32m~/hiddeninfo/.env/lib/python3.9/site-packages/torch/serialization.py:771\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    769\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 771\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    772\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    773\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    774\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    775\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    776\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/hiddeninfo/.env/lib/python3.9/site-packages/torch/serialization.py:270\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 270\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    271\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    272\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/hiddeninfo/.env/lib/python3.9/site-packages/torch/serialization.py:251\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/big_run/seq_inst/20230224-124623/final_model.pth'"
     ]
    }
   ],
   "source": [
    "# Get attribute vectors across different models\n",
    "\n",
    "ind_inst = []\n",
    "seq_attrs = []\n",
    "seq_sparse_attrs = []\n",
    "joint_attrs = []\n",
    "multi_attrs = []\n",
    "\n",
    "model_paths = [seq_model_path, seq_sparse_model_path, model_with_two_joints_path, multi_path]\n",
    "output_lists = [seq_attrs, seq_sparse_attrs, joint_attrs, multi_attrs]\n",
    "with torch.no_grad():\n",
    "    for model_path, olist in zip(model_paths, output_lists):\n",
    "        print(model_path)\n",
    "        model = torch.load(model_path)\n",
    "        model.eval()\n",
    "        for batch in train_loader:\n",
    "            inputs, class_labels, attr_labels, attr_mask = batch\n",
    "\n",
    "            attr_labels = [i.float() for i in attr_labels]\n",
    "            attr_labels = torch.stack(attr_labels, dim=1)\n",
    "\n",
    "            attr_labels = attr_labels.cuda() if torch.cuda.is_available() else attr_labels\n",
    "            inputs = inputs.cuda() if torch.cuda.is_available() else inputs\n",
    "            class_labels = class_labels.cuda() if torch.cuda.is_available() else class_labels\n",
    "            attr_mask = attr_mask.cuda() if torch.cuda.is_available() else attr_mask\n",
    "\n",
    "            output = model.generate_predictions(inputs, attr_labels, attr_mask)\n",
    "            olist.append(output[0])\n",
    "\n",
    "    del(model) # Clear memory\n",
    "    print(f\"Done with model {model_path.split('/')[-3]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a8b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the difference between the two joint models\n",
    "seq_attrs = seq_attrs[0]\n",
    "seq_sparse_attrs = seq_sparse_attrs[0]\n",
    "joint_attrs1 = joint_attrs[0]\n",
    "joint_attrs2 = joint_attrs[1]\n",
    "multi_attrs1 = multi_attrs[0]\n",
    "multi_attrs2 = multi_attrs[1]\n",
    "\n",
    "diff1 = joint_attrs1 - seq_attrs\n",
    "diff2 = joint_attrs2 - seq_attrs\n",
    "\n",
    "cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "\n",
    "print(\"Cosine similarity between joint1 and seq\")\n",
    "print(cos_sim(joint_attrs1, seq_attrs).mean())\n",
    "print(\"Cosine similarity between joint2 and seq\")\n",
    "print(cos_sim(joint_attrs2, seq_attrs).mean())\n",
    "print(\"Cosime similiary of the deviations from seq\")\n",
    "print(cos_sim(diff1, diff2).mean())\n",
    "print(\"Cosine similarity of seq and seq_sparse\")\n",
    "print(cos_sim(seq_attrs, seq_sparse_attrs).mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6 (default, Oct 18 2022, 12:41:40) \n[Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "4b95b6b691c30533c324664bd6a9848f5f6465bdf018a5870117d4007cfdc865"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
