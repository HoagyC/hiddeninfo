{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " So the overall plan is something like:\n",
    "\n",
    " We want to be able to train a model, where the neuron activations have a particular signal, which guides them to activate according to a particular feature.\n",
    " There are two main ways that we can train this feature - we either give a training signal which is purely the signal\n",
    "\n",
    " Benefits of pure training:\n",
    " Confidence that the network is intending to mean what we want it to mean\n",
    "\n",
    " Benefits of mixed training:\n",
    " Allows the use of the downstream signal to disambiguate between possible generalizations of the concept that we want.\n",
    " This was the initial motivation in the ELK lens - we use the downstream training signal to push the question answerer towards the direct reporter, rather than the human simulator, because the former is more useful on the downstream tasks.\n",
    "\n",
    " This should also show up in working better with sparse labels, which we expect to be very useful since manual labels are very expensive (though automating the labelling will be necessary and might make this less useful). \n",
    "\n",
    "\n",
    " My hope was that if we trained separate Joint models, and then 'crossed the wires', so the image -> concept part of model 1 was passing its outputs to the concept -> class function of model 2 50% of the time, while 50% it still passed it to model 1, and these concept -> image model were frozen, the model would be forced to switch to a concept representation that matched to only that which the models had in common.\n",
    "\n",
    " We can see this in the clustering of the results here. Looks like there's \n",
    "\n",
    " We can show this \n",
    "\n",
    " Aside: Can we look at this through the lens of single-basin theory and see which attributes match up well to something that arises in the non-attribute loss case.?\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " We started off by doing toy models in simple domains. The original idea came from autoencoders, where we would encode\n",
    "\n",
    " We see this in our early experiments using autoencoders, though it would be good to be back try to demonstrate this in a feed-forward setting - I ran some basic experiments on this but it wasn't working well - this might be a serious issue but it's not the issue faced by the experiments at the moment. Hypothesis - it depends on the relative complexity of the pre and post true functions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I moved on to getting this system working with the CUB dataset. This is a dataset which contains >10K images of birds, which are each labelled as being one of 200 species of bird. They are also given binary labels for 312 features which describe the birds' appearances. These categories are divided into 28 categories, such as beak-shape, and only one of these categories are true for any image.\n",
    "\n",
    "Although one would expect that the actual characteristics of the bird are the same for each bird, the labels differ significantly between birds, and labels are also often marked not-visible. As a result of this, most papers which use the CUB dataset make the task  easier by reducing the number of categories to those which are true in at least 10 cases, which is around 110 (109 in my own experiments, 112 in the original). They also make it much easier by doing a majority-voting transform, by which the labels for each image are replaced by the most common category forbirdsw of that species, within the test set.\n",
    "\n",
    "This second transform not only makes the "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
